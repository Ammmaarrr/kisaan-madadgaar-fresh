{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c1b8ae1",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) - Plant Disease Detection\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis on the PlantVillage dataset:\n",
    "1. **Dataset Overview** - Structure, size, classes\n",
    "2. **Class Distribution** - Balance analysis\n",
    "3. **Image Properties** - Dimensions, channels, statistics\n",
    "4. **Sample Visualization** - View images from each class\n",
    "5. **Data Quality** - Missing values, corrupted files\n",
    "6. **Preprocessing Requirements** - Recommendations\n",
    "\n",
    "---\n",
    "\n",
    "## Authors: 1.MUHAMMAD AMMAR 2. ABDUL HAKEEM            \n",
    "## Date: November 2025\n",
    "## Course: Artificial Intelligence - Progress Report II\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec3a458",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2458c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Add parent directory to path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import preprocessing modules\n",
    "from preprocessing import DataPreprocessingPipeline, DataAugmentation\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Import utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('Set2')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489497c7",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc2261b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Configuration:\n",
      "  img_size: 224\n",
      "  batch_size: 32\n",
      "  augmentation: True\n",
      "  train_ratio: 0.7\n",
      "  val_ratio: 0.15\n",
      "  test_ratio: 0.15\n",
      "  num_workers: 4\n",
      "\n",
      "üìÅ Data Path: ../data/PlantVillage\n",
      "üìÅ Results Directory: ../results/eda\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    'img_size': 224,\n",
    "    'batch_size': 32,\n",
    "    'augmentation': True,\n",
    "    'train_ratio': 0.7,\n",
    "    'val_ratio': 0.15,\n",
    "    'test_ratio': 0.15,\n",
    "    'num_workers': 4\n",
    "}\n",
    "\n",
    "# Data path - PAKISTAN UNIFIED DATASET (34 disease classes)\n",
    "data_path = '../data/PakistanCrops_Merged'  # Rice, Cotton, Wheat, Mango + PlantVillage crops\n",
    "\n",
    "# Results directory\n",
    "results_dir = '../results/eda'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "print(\"üìã Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"\\nüìÅ Data Path: {data_path}\")\n",
    "print(f\"üìÅ Results Directory: {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c77c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "üì• Downloading PlantDisease dataset from Kaggle...\n",
      "‚úÖ Dataset downloaded successfully!\n",
      "üìÅ Path to dataset files: C:\\Users\\PC\\.cache\\kagglehub\\datasets\\emmarex\\plantdisease\\versions\\1\n",
      "\n",
      "üîÑ Updated data_path to: C:\\Users\\PC\\.cache\\kagglehub\\datasets\\emmarex\\plantdisease\\versions\\1\n",
      "\n",
      "üìÇ Dataset contents:\n",
      "  üìÅ PlantVillage/ (16 files)\n"
     ]
    }
   ],
   "source": [
    "# ‚ö†Ô∏è KAGGLE DOWNLOAD DISABLED - Using Pakistan Unified Dataset\n",
    "# The Pakistan dataset is already prepared with 46,808 images across 34 classes\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìÅ USING PAKISTAN UNIFIED DATASET (34 CLASSES)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"‚úÖ Dataset location: {data_path}\")\n",
    "print(\"üìä This dataset includes Rice, Cotton, Wheat, Mango + PlantVillage crops\")\n",
    "print(\"üìä Specifically optimized for Pakistani agricultural context\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Verify path exists\n",
    "if os.path.exists(data_path):\n",
    "    print(f\"‚úÖ Dataset path verified: {data_path}\")\n",
    "    # List subdirectories (disease classes)\n",
    "    subdirs = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\n",
    "    print(f\"üìÇ Found {len(subdirs)} disease classes\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Warning: Dataset path not found: {data_path}\")\n",
    "    print(\"Please ensure the Pakistan dataset exists at the specified location.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87979b3c",
   "metadata": {},
   "source": [
    "## 2.1 Download Kaggle Dataset (Optional)\n",
    "\n",
    "If you want to use the real PlantDisease dataset from Kaggle, run this cell.\n",
    "This will download the dataset automatically using kagglehub.\n",
    "\n",
    "**Note:** You need to have Kaggle API credentials set up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4fc43a",
   "metadata": {},
   "source": [
    "## 3. Initialize Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9a0288e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing.data_pipeline:Transforms configured successfully\n",
      "INFO:preprocessing.data_pipeline:DataPreprocessingPipeline initialized: img_size=224, batch_size=32, augmentation=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data preprocessing pipeline initialized\n",
      "\n",
      "üìä Pipeline Configuration:\n",
      "  Image Size: 224x224\n",
      "  Batch Size: 32\n",
      "  Augmentation: True\n",
      "  Train/Val/Test Split: 0.7/0.15/0.15\n"
     ]
    }
   ],
   "source": [
    "# Initialize preprocessing pipeline\n",
    "pipeline = DataPreprocessingPipeline(config)\n",
    "\n",
    "print(\"‚úÖ Data preprocessing pipeline initialized\")\n",
    "print(f\"\\nüìä Pipeline Configuration:\")\n",
    "print(f\"  Image Size: {pipeline.img_size}x{pipeline.img_size}\")\n",
    "print(f\"  Batch Size: {pipeline.batch_size}\")\n",
    "print(f\"  Augmentation: {pipeline.augmentation_enabled}\")\n",
    "print(f\"  Train/Val/Test Split: {pipeline.train_ratio}/{pipeline.val_ratio}/{pipeline.test_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eb27ea",
   "metadata": {},
   "source": [
    "## 4. Load Dataset\n",
    "\n",
    "**Note:** If you don't have the dataset yet, you can:\n",
    "1. Download PlantVillage dataset from Kaggle\n",
    "2. Or use the test_images from the repository for demonstration\n",
    "3. Expected structure:\n",
    "   ```\n",
    "   data/PlantVillage/\n",
    "       Apple___Apple_scab/\n",
    "           image1.jpg\n",
    "           image2.jpg\n",
    "       Tomato___Bacterial_spot/\n",
    "           image1.jpg\n",
    "           image2.jpg\n",
    "       ...\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60727deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing.data_pipeline:Loaded from directory: 1 classes, 0 images\n",
      "INFO:preprocessing.data_pipeline:Loaded 0 images from 1 classes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data directory found: C:\\Users\\PC\\.cache\\kagglehub\\datasets\\emmarex\\plantdisease\\versions\\1\n",
      "\n",
      "‚úÖ Data loaded successfully!\n",
      "Total Images: 0\n",
      "Number of Classes: 1\n"
     ]
    }
   ],
   "source": [
    "# Check if data path exists\n",
    "if os.path.exists(data_path):\n",
    "    print(f\"‚úÖ Data directory found: {data_path}\")\n",
    "    \n",
    "    # Load data\n",
    "    try:\n",
    "        pipeline.load_data(data_path, structure='directory')\n",
    "        print(f\"\\n‚úÖ Data loaded successfully!\")\n",
    "        print(f\"Total Images: {len(pipeline.image_paths)}\")\n",
    "        print(f\"Number of Classes: {len(pipeline.class_names)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        print(\"\\nüí° Creating mock dataset for demonstration...\")\n",
    "        # Create mock data for demonstration\n",
    "        pipeline.class_names = ['Apple___Apple_scab', 'Tomato___Bacterial_spot', \n",
    "                                'Potato___Late_blight', 'Corn___Common_rust']\n",
    "        pipeline.image_paths = [f'image_{i}.jpg' for i in range(1000)]\n",
    "        pipeline.labels = np.random.randint(0, 4, 1000).tolist()\n",
    "        print(\"‚úÖ Mock dataset created for demonstration\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Data directory not found: {data_path}\")\n",
    "    print(\"\\nüí° Creating mock dataset for demonstration...\")\n",
    "    \n",
    "    # Create mock data for demonstration\n",
    "    pipeline.class_names = [\n",
    "        'Apple___Apple_scab',\n",
    "        'Apple___Black_rot',\n",
    "        'Apple___Cedar_apple_rust',\n",
    "        'Apple___healthy',\n",
    "        'Tomato___Bacterial_spot',\n",
    "        'Tomato___Early_blight',\n",
    "        'Tomato___Late_blight',\n",
    "        'Tomato___healthy',\n",
    "        'Potato___Early_blight',\n",
    "        'Potato___Late_blight',\n",
    "        'Potato___healthy'\n",
    "    ]\n",
    "    \n",
    "    # Generate mock data with realistic distribution\n",
    "    num_images = 5000\n",
    "    pipeline.image_paths = [f'mock_image_{i}.jpg' for i in range(num_images)]\n",
    "    \n",
    "    # Create slightly imbalanced distribution (realistic)\n",
    "    weights = np.array([1.0, 0.9, 0.8, 1.2, 1.1, 0.95, 0.85, 1.3, 1.0, 0.9, 1.1])\n",
    "    weights = weights / weights.sum()\n",
    "    pipeline.labels = np.random.choice(len(pipeline.class_names), num_images, p=weights).tolist()\n",
    "    \n",
    "    pipeline.class_to_idx = {name: idx for idx, name in enumerate(pipeline.class_names)}\n",
    "    \n",
    "    print(\"‚úÖ Mock dataset created for demonstration\")\n",
    "    print(f\"Total Images: {len(pipeline.image_paths)}\")\n",
    "    print(f\"Number of Classes: {len(pipeline.class_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6505a3ee",
   "metadata": {},
   "source": [
    "## 5. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e3d7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå± Plant Disease Classes:\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müå± Plant Disease Classes:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, class_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pipeline.class_names, \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     plant, disease = class_name.split(\u001b[33m'\u001b[39m\u001b[33m___\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m2d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplant\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m15s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisease\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 2, got 1)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Display class names\n",
    "print(\"üå± Plant Disease Classes:\\n\")\n",
    "for i, class_name in enumerate(pipeline.class_names, 1):\n",
    "    plant, disease = class_name.split('___')\n",
    "    print(f\"{i:2d}. {plant:15s} - {disease}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9672193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Class Distribution:\n",
      "\n",
      "                   Class  Count Percentage\n",
      "        Tomato___healthy    622     12.44%\n",
      "        Potato___healthy    552     11.04%\n",
      "         Apple___healthy    526     10.52%\n",
      " Tomato___Bacterial_spot    500     10.00%\n",
      "   Potato___Early_blight    435      8.70%\n",
      "   Tomato___Early_blight    424      8.48%\n",
      "      Apple___Apple_scab    421      8.42%\n",
      "    Tomato___Late_blight    412      8.24%\n",
      "    Potato___Late_blight    394      7.88%\n",
      "       Apple___Black_rot    393      7.86%\n",
      "Apple___Cedar_apple_rust    321      6.42%\n",
      "\n",
      "üìà Statistics:\n",
      "  Total Images: 5000\n",
      "  Mean per class: 454.55\n",
      "  Std per class: 82.16\n",
      "  Min: 321\n",
      "  Max: 622\n",
      "  Balance Ratio (Min/Max): 0.52\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Class distribution\n",
    "label_counts = Counter(pipeline.labels)\n",
    "class_distribution = {\n",
    "    pipeline.class_names[label]: count \n",
    "    for label, count in label_counts.items()\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df_dist = pd.DataFrame([\n",
    "    {'Class': class_name, 'Count': count, 'Percentage': f\"{count/len(pipeline.labels)*100:.2f}%\"}\n",
    "    for class_name, count in class_distribution.items()\n",
    "]).sort_values('Count', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Class Distribution:\\n\")\n",
    "print(df_dist.to_string(index=False))\n",
    "\n",
    "print(f\"\\nüìà Statistics:\")\n",
    "print(f\"  Total Images: {len(pipeline.labels)}\")\n",
    "print(f\"  Mean per class: {np.mean(list(label_counts.values())):.2f}\")\n",
    "print(f\"  Std per class: {np.std(list(label_counts.values())):.2f}\")\n",
    "print(f\"  Min: {min(label_counts.values())}\")\n",
    "print(f\"  Max: {max(label_counts.values())}\")\n",
    "print(f\"  Balance Ratio (Min/Max): {min(label_counts.values())/max(label_counts.values()):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583eb16e",
   "metadata": {},
   "source": [
    "## 6. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd54f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution bar plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "classes = [c.replace('___', '\\n') for c in df_dist['Class']]\n",
    "counts = df_dist['Count'].values\n",
    "\n",
    "bars = plt.bar(range(len(classes)), counts, color='steelblue', alpha=0.8, edgecolor='black')\n",
    "\n",
    "# Color the bars based on count (highlight min and max)\n",
    "max_idx = counts.argmax()\n",
    "min_idx = counts.argmin()\n",
    "bars[max_idx].set_color('green')\n",
    "bars[min_idx].set_color('red')\n",
    "\n",
    "plt.xlabel('Disease Class', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Number of Images', fontsize=12, fontweight='bold')\n",
    "plt.title('Class Distribution - PlantVillage Dataset', fontsize=14, fontweight='bold')\n",
    "plt.xticks(range(len(classes)), classes, rotation=45, ha='right', fontsize=9)\n",
    "plt.axhline(y=np.mean(counts), color='orange', linestyle='--', linewidth=2, \n",
    "            label=f'Mean: {np.mean(counts):.0f}')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, 'class_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Class distribution plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee59da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart for class proportions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Pie chart\n",
    "colors = sns.color_palette('Set3', len(classes))\n",
    "ax1.pie(counts, labels=classes, autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "ax1.set_title('Class Proportion', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Box plot\n",
    "ax2.boxplot([counts], vert=True, patch_artist=True,\n",
    "            boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "            medianprops=dict(color='red', linewidth=2))\n",
    "ax2.set_ylabel('Number of Images', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Class Distribution Statistics', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.set_xticklabels(['All Classes'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, 'class_statistics.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Class statistics plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da82ac1a",
   "metadata": {},
   "source": [
    "## 7. Data Balance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b5b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate balance metrics\n",
    "balance_ratio = min(counts) / max(counts)\n",
    "imbalance_severity = 1 - balance_ratio\n",
    "\n",
    "print(\"‚öñÔ∏è  Data Balance Analysis:\\n\")\n",
    "print(f\"Balance Ratio: {balance_ratio:.3f}\")\n",
    "print(f\"Imbalance Severity: {imbalance_severity:.3f}\")\n",
    "\n",
    "if balance_ratio >= 0.9:\n",
    "    print(\"\\n‚úÖ Dataset is well-balanced\")\n",
    "    recommendation = \"No special handling needed\"\n",
    "elif balance_ratio >= 0.7:\n",
    "    print(\"\\n‚ö†Ô∏è  Dataset is slightly imbalanced\")\n",
    "    recommendation = \"Consider using class weights in loss function\"\n",
    "elif balance_ratio >= 0.5:\n",
    "    print(\"\\n‚ö†Ô∏è  Dataset is moderately imbalanced\")\n",
    "    recommendation = \"Use class weights + data augmentation for minority classes\"\n",
    "else:\n",
    "    print(\"\\n‚ùå Dataset is severely imbalanced\")\n",
    "    recommendation = \"Use class weights + oversampling/undersampling + data augmentation\"\n",
    "\n",
    "print(f\"\\nüí° Recommendation: {recommendation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d1799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights for handling imbalance\n",
    "if len(pipeline.labels) > 0:\n",
    "    try:\n",
    "        import torch\n",
    "        class_weights = pipeline.get_class_weights()\n",
    "        \n",
    "        print(\"\\n‚öñÔ∏è  Calculated Class Weights (for loss function):\\n\")\n",
    "        for i, (class_name, weight) in enumerate(zip(pipeline.class_names, class_weights)):\n",
    "            print(f\"{class_name:40s}: {weight:.4f}\")\n",
    "    except:\n",
    "        print(\"\\n‚ö†Ô∏è  PyTorch not available for class weight calculation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4185b6bb",
   "metadata": {},
   "source": [
    "## 8. Train/Val/Test Split Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0ee6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate split sizes\n",
    "total_images = len(pipeline.labels)\n",
    "train_size = int(pipeline.train_ratio * total_images)\n",
    "val_size = int(pipeline.val_ratio * total_images)\n",
    "test_size = total_images - train_size - val_size\n",
    "\n",
    "print(\"üìä Data Split Configuration:\\n\")\n",
    "print(f\"Total Images: {total_images}\")\n",
    "print(f\"\\nTrain Set: {train_size} ({pipeline.train_ratio*100:.0f}%)\")\n",
    "print(f\"Val Set:   {val_size} ({pipeline.val_ratio*100:.0f}%)\")\n",
    "print(f\"Test Set:  {test_size} ({pipeline.test_ratio*100:.0f}%)\")\n",
    "\n",
    "# Visualize split\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "splits = ['Train', 'Validation', 'Test']\n",
    "sizes = [train_size, val_size, test_size]\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(sizes, labels=splits, autopct='%1.1f%%',\n",
    "                                    colors=colors, startangle=90,\n",
    "                                    explode=(0.05, 0.05, 0.05))\n",
    "\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontsize(12)\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "ax.set_title('Train/Validation/Test Split', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add legend with actual numbers\n",
    "legend_labels = [f\"{split}: {size} images\" for split, size in zip(splits, sizes)]\n",
    "ax.legend(legend_labels, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, 'data_split.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Data split visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9c9cfe",
   "metadata": {},
   "source": [
    "## 9. Data Augmentation Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4096d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize augmentation\n",
    "augmentation = DataAugmentation({\n",
    "    'rotation_range': 15,\n",
    "    'flip_probability': 0.5,\n",
    "    'brightness_range': 0.2,\n",
    "    'contrast_range': 0.2,\n",
    "    'saturation_range': 0.2,\n",
    "    'hue_range': 0.1,\n",
    "    'zoom_range': (0.8, 1.0)\n",
    "})\n",
    "\n",
    "print(\"üé® Data Augmentation Configuration:\\n\")\n",
    "aug_config = augmentation.get_augmentation_config()\n",
    "for key, value in aug_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e1c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize augmentation effects\n",
    "print(\"\\nüìù Note: To visualize augmentation effects, you would need actual images.\")\n",
    "print(\"The augmentation pipeline includes:\")\n",
    "print(\"  ‚úÖ Random rotation (¬±15¬∞)\")\n",
    "print(\"  ‚úÖ Random horizontal flip (50%)\")\n",
    "print(\"  ‚úÖ Color jitter (brightness, contrast, saturation, hue)\")\n",
    "print(\"  ‚úÖ Random resized crop (80-100% scale)\")\n",
    "print(\"  ‚úÖ Gaussian blur (20% probability)\")\n",
    "print(\"  ‚úÖ Random affine transformations (30% probability)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d531801f",
   "metadata": {},
   "source": [
    "## 10. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc9633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Data Quality Assessment:\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"1. Missing Values:\")\n",
    "missing_paths = sum(1 for p in pipeline.image_paths if not p)\n",
    "missing_labels = sum(1 for l in pipeline.labels if l is None)\n",
    "print(f\"   Missing image paths: {missing_paths}\")\n",
    "print(f\"   Missing labels: {missing_labels}\")\n",
    "\n",
    "# Check label distribution\n",
    "print(\"\\n2. Label Distribution:\")\n",
    "unique_labels = len(set(pipeline.labels))\n",
    "print(f\"   Unique classes: {unique_labels} / {len(pipeline.class_names)}\")\n",
    "\n",
    "# Check for potential issues\n",
    "print(\"\\n3. Potential Issues:\")\n",
    "issues = []\n",
    "if balance_ratio < 0.7:\n",
    "    issues.append(\"‚ö†Ô∏è  Class imbalance detected\")\n",
    "if len(pipeline.labels) < 1000:\n",
    "    issues.append(\"‚ö†Ô∏è  Small dataset size\")\n",
    "if unique_labels < len(pipeline.class_names):\n",
    "    issues.append(\"‚ö†Ô∏è  Some classes have no samples\")\n",
    "\n",
    "if issues:\n",
    "    for issue in issues:\n",
    "        print(f\"   {issue}\")\n",
    "else:\n",
    "    print(\"   ‚úÖ No major issues detected\")\n",
    "\n",
    "# Overall quality score\n",
    "quality_score = 100\n",
    "if balance_ratio < 0.7:\n",
    "    quality_score -= 20\n",
    "if len(pipeline.labels) < 1000:\n",
    "    quality_score -= 15\n",
    "if unique_labels < len(pipeline.class_names):\n",
    "    quality_score -= 25\n",
    "\n",
    "print(f\"\\nüìä Overall Data Quality Score: {quality_score}/100\")\n",
    "\n",
    "if quality_score >= 90:\n",
    "    print(\"‚úÖ Excellent data quality\")\n",
    "elif quality_score >= 75:\n",
    "    print(\"‚úÖ Good data quality\")\n",
    "elif quality_score >= 60:\n",
    "    print(\"‚ö†Ô∏è  Fair data quality - improvements recommended\")\n",
    "else:\n",
    "    print(\"‚ùå Poor data quality - significant improvements needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d95da",
   "metadata": {},
   "source": [
    "## 11. Recommendations for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba9386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üí° Recommendations for Model Training:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Based on dataset size\n",
    "if len(pipeline.labels) < 5000:\n",
    "    recommendations.append({\n",
    "        'category': 'üìä Dataset Size',\n",
    "        'recommendation': 'Use transfer learning with pre-trained models (ResNet, VGG)',\n",
    "        'reason': f'Dataset has {len(pipeline.labels)} images - transfer learning will help'\n",
    "    })\n",
    "    recommendations.append({\n",
    "        'category': 'üé® Augmentation',\n",
    "        'recommendation': 'Use heavy data augmentation',\n",
    "        'reason': 'Small dataset benefits from extensive augmentation'\n",
    "    })\n",
    "else:\n",
    "    recommendations.append({\n",
    "        'category': 'üìä Dataset Size',\n",
    "        'recommendation': 'Can train models from scratch or use transfer learning',\n",
    "        'reason': f'Dataset has {len(pipeline.labels)} images - sufficient for training'\n",
    "    })\n",
    "\n",
    "# Based on class imbalance\n",
    "if balance_ratio < 0.7:\n",
    "    recommendations.append({\n",
    "        'category': '‚öñÔ∏è  Class Balance',\n",
    "        'recommendation': 'Use class weights in loss function',\n",
    "        'reason': f'Balance ratio is {balance_ratio:.2f} - imbalanced classes detected'\n",
    "    })\n",
    "    recommendations.append({\n",
    "        'category': 'üéØ Sampling',\n",
    "        'recommendation': 'Consider oversampling minority classes',\n",
    "        'reason': 'Will help balance training data'\n",
    "    })\n",
    "\n",
    "# Training strategy\n",
    "recommendations.append({\n",
    "    'category': 'üîß Training Strategy',\n",
    "    'recommendation': 'Use learning rate scheduling and early stopping',\n",
    "    'reason': 'Prevents overfitting and improves convergence'\n",
    "})\n",
    "\n",
    "recommendations.append({\n",
    "    'category': 'üìà Evaluation',\n",
    "    'recommendation': 'Monitor per-class metrics (precision, recall, F1)',\n",
    "    'reason': 'Important for imbalanced datasets'\n",
    "})\n",
    "\n",
    "# Display recommendations\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"\\n{i}. {rec['category']}\")\n",
    "    print(f\"   Recommendation: {rec['recommendation']}\")\n",
    "    print(f\"   Reason: {rec['reason']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f90553a",
   "metadata": {},
   "source": [
    "## 12. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15afff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "summary = {\n",
    "    'Dataset Overview': {\n",
    "        'Total Images': len(pipeline.labels),\n",
    "        'Number of Classes': len(pipeline.class_names),\n",
    "        'Train Set': f\"{train_size} ({pipeline.train_ratio*100:.0f}%)\",\n",
    "        'Val Set': f\"{val_size} ({pipeline.val_ratio*100:.0f}%)\",\n",
    "        'Test Set': f\"{test_size} ({pipeline.test_ratio*100:.0f}%)\"\n",
    "    },\n",
    "    'Class Distribution': {\n",
    "        'Mean Images per Class': f\"{np.mean(list(label_counts.values())):.2f}\",\n",
    "        'Std Images per Class': f\"{np.std(list(label_counts.values())):.2f}\",\n",
    "        'Min Images': min(label_counts.values()),\n",
    "        'Max Images': max(label_counts.values()),\n",
    "        'Balance Ratio': f\"{balance_ratio:.3f}\"\n",
    "    },\n",
    "    'Configuration': {\n",
    "        'Image Size': f\"{config['img_size']}x{config['img_size']}\",\n",
    "        'Batch Size': config['batch_size'],\n",
    "        'Augmentation': 'Enabled' if config['augmentation'] else 'Disabled',\n",
    "        'Num Workers': config['num_workers']\n",
    "    },\n",
    "    'Data Quality': {\n",
    "        'Quality Score': f\"{quality_score}/100\",\n",
    "        'Missing Values': missing_paths + missing_labels,\n",
    "        'Issues Detected': len(issues)\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nüìã COMPREHENSIVE EDA SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for section, metrics in summary.items():\n",
    "    print(f\"\\n{section}:\")\n",
    "    print(\"-\" * 50)\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"  {key:30s}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Exploratory Data Analysis Complete!\")\n",
    "print(f\"üìÅ Results saved to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79f3949",
   "metadata": {},
   "source": [
    "## 13. Export Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b066b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary to JSON\n",
    "import json\n",
    "\n",
    "summary_file = os.path.join(results_dir, 'eda_summary.json')\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Summary saved to: {summary_file}\")\n",
    "\n",
    "# Save class distribution to CSV\n",
    "csv_file = os.path.join(results_dir, 'class_distribution.csv')\n",
    "df_dist.to_csv(csv_file, index=False)\n",
    "print(f\"‚úÖ Class distribution saved to: {csv_file}\")\n",
    "\n",
    "print(\"\\nüéì EDA complete and ready for Progress Report II!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
