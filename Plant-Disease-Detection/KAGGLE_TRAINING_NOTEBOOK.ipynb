{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df45a97",
   "metadata": {},
   "source": [
    "## 1. Check Available Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc2366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# On Kaggle, datasets are automatically mounted at /kaggle/input/\n",
    "kaggle_input = '/kaggle/input'\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ“Š CHECKING AVAILABLE DATASETS ON KAGGLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if os.path.exists(kaggle_input):\n",
    "    print(\"\\nâœ… Kaggle environment detected\\n\")\n",
    "    print(\"ğŸ“ Available datasets:\")\n",
    "    for item in os.listdir(kaggle_input):\n",
    "        item_path = os.path.join(kaggle_input, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            # Count files\n",
    "            file_count = sum([len(files) for _, _, files in os.walk(item_path)])\n",
    "            print(f\"  âœ“ {item:50s} ({file_count:,} files)\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Not on Kaggle. Upload this notebook to Kaggle to access datasets.\")\n",
    "    print(\"   Go to: https://www.kaggle.com/code\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39ddf32",
   "metadata": {},
   "source": [
    "## 2. Install Required Packages + Speed Optimizations\n",
    "\n",
    "**âš¡ Optimizations enabled:**\n",
    "- Mixed Precision (FP16) training\n",
    "- cuDNN benchmark mode\n",
    "- Parallel data loading (4 workers)\n",
    "- Pin memory for faster GPU transfer\n",
    "- Batch prefetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e656532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional packages if needed\n",
    "!pip install timm efficientnet-pytorch torchmetrics -q\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import timm\n",
    "\n",
    "# âš¡ OPTIMIZATION: Enable cuDNN benchmarking for faster convolutions\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "# âš¡ OPTIMIZATION: Set up mixed precision training\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler = GradScaler()\n",
    "\n",
    "print(\"âœ… All packages imported successfully\")\n",
    "print(f\"ğŸ–¥ï¸  Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n",
    "print(f\"ğŸ® CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"âš¡ cuDNN benchmark: {torch.backends.cudnn.benchmark}\")\n",
    "print(f\"âš¡ Mixed Precision (AMP): Enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1e7e6e",
   "metadata": {},
   "source": [
    "## 3. Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28640e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map dataset names to expected Kaggle paths\n",
    "dataset_mapping = {\n",
    "    'rice': 'rice-diseases-image-dataset',\n",
    "    'cotton': 'cotton-disease-dataset', \n",
    "    'wheat': 'wheat-leaf-dataset',\n",
    "    'mango': 'mango-leaf-disease-dataset',\n",
    "    'plantvillage': 'plantdisease'\n",
    "}\n",
    "\n",
    "# Find actual paths\n",
    "data_paths = {}\n",
    "for crop, expected_name in dataset_mapping.items():\n",
    "    # Check various possible names\n",
    "    for item in os.listdir('/kaggle/input'):\n",
    "        if expected_name.lower() in item.lower():\n",
    "            data_paths[crop] = os.path.join('/kaggle/input', item)\n",
    "            break\n",
    "\n",
    "print(\"ğŸ“¦ Dataset Paths:\")\n",
    "for crop, path in data_paths.items():\n",
    "    status = \"âœ…\" if os.path.exists(path) else \"âŒ\"\n",
    "    print(f\"  {status} {crop.upper():15s}: {path}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Total datasets found: {len(data_paths)}/{len(dataset_mapping)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724b8afb",
   "metadata": {},
   "source": [
    "## 4. Load and Analyze Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bebb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_image_classes(base_path, max_depth=5):\n",
    "    \"\"\"Find all image class directories\"\"\"\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG'}\n",
    "    class_data = {}\n",
    "    \n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        # Check depth\n",
    "        depth = root[len(base_path):].count(os.sep)\n",
    "        if depth > max_depth:\n",
    "            continue\n",
    "            \n",
    "        image_files = [f for f in files if Path(f).suffix in image_extensions]\n",
    "        if image_files and len(image_files) > 50:  # At least 50 images\n",
    "            class_name = Path(root).name\n",
    "            if class_name not in class_data:\n",
    "                class_data[class_name] = []\n",
    "            class_data[class_name].extend([os.path.join(root, f) for f in image_files])\n",
    "    \n",
    "    return class_data\n",
    "\n",
    "# Analyze all datasets\n",
    "all_classes = {}\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š ANALYZING DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for crop, path in data_paths.items():\n",
    "    if os.path.exists(path):\n",
    "        print(f\"\\nğŸ” Analyzing {crop.upper()}...\")\n",
    "        classes = find_image_classes(path)\n",
    "        all_classes[crop] = classes\n",
    "        print(f\"   Found: {len(classes)} classes, \"\n",
    "              f\"{sum(len(imgs) for imgs in classes.values()):,} images\")\n",
    "\n",
    "total_classes = sum(len(classes) for classes in all_classes.values())\n",
    "total_images = sum(len(imgs) for crop_classes in all_classes.values() \n",
    "                   for imgs in crop_classes.values())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"ğŸ“ˆ TOTAL: {total_classes} classes, {total_images:,} images\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a41805",
   "metadata": {},
   "source": [
    "## 5. Create Unified Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d5d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PakistanCropDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None, class_names=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.class_names = class_names or []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Create unified dataset\n",
    "all_image_paths = []\n",
    "all_labels = []\n",
    "class_names = []\n",
    "class_to_idx = {}\n",
    "\n",
    "print(\"\\nğŸ”¨ Creating unified dataset...\")\n",
    "\n",
    "current_idx = 0\n",
    "for crop, classes in all_classes.items():\n",
    "    for class_name, image_paths in classes.items():\n",
    "        if len(image_paths) < 100:  # Skip small classes\n",
    "            continue\n",
    "        \n",
    "        standardized_name = f\"{crop.capitalize()}___{class_name.replace(' ', '_')}\"\n",
    "        class_names.append(standardized_name)\n",
    "        class_to_idx[standardized_name] = current_idx\n",
    "        \n",
    "        all_image_paths.extend(image_paths)\n",
    "        all_labels.extend([current_idx] * len(image_paths))\n",
    "        \n",
    "        current_idx += 1\n",
    "\n",
    "num_classes = len(class_names)\n",
    "print(f\"âœ… Dataset created: {num_classes} classes, {len(all_image_paths):,} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b8d4e6",
   "metadata": {},
   "source": [
    "## 6. Data Transforms and Loaders (âš¡ OPTIMIZED)\n",
    "\n",
    "**Speed optimizations:**\n",
    "- `batch_size=64` (doubled)\n",
    "- `num_workers=4` (parallel loading)  \n",
    "- `pin_memory=True` (faster GPU transfer)\n",
    "- `prefetch_factor=2` (preload batches)\n",
    "- `persistent_workers=True` (keep workers alive)\n",
    "- Lighter augmentation (removed ColorJitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fcaa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš¡ OPTIMIZED Image transformations - lighter augmentation for speed\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Standard size for EfficientNet\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),  # Reduced from 15 for speed\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create full dataset\n",
    "full_dataset = PakistanCropDataset(all_image_paths, all_labels, transform=train_transform, class_names=class_names)\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = int(0.15 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, \n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Apply test transform to val and test\n",
    "val_dataset.dataset.transform = test_transform\n",
    "test_dataset.dataset.transform = test_transform\n",
    "\n",
    "# âš¡ OPTIMIZED: Larger batch size + more workers + pin_memory + prefetch\n",
    "batch_size = 64  # Increased from 32 - T4 can handle this easily\n",
    "num_workers = 4  # âš¡ KEY FIX: Was 2, now 4 for parallel data loading\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,           # âš¡ Faster GPU transfer\n",
    "    prefetch_factor=2,         # âš¡ Prefetch next batches\n",
    "    persistent_workers=True    # âš¡ Keep workers alive between epochs\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size * 2,  # âš¡ Larger batch for validation (no gradients)\n",
    "    shuffle=False, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=2,\n",
    "    persistent_workers=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size * 2, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset Split:\")\n",
    "print(f\"  Training:   {len(train_dataset):,} images\")\n",
    "print(f\"  Validation: {len(val_dataset):,} images\")\n",
    "print(f\"  Test:       {len(test_dataset):,} images\")\n",
    "print(f\"\\nâš¡ OPTIMIZATIONS APPLIED:\")\n",
    "print(f\"  Batch size: {batch_size} (train) / {batch_size*2} (val/test)\")\n",
    "print(f\"  Workers: {num_workers}\")\n",
    "print(f\"  Pin memory: True\")\n",
    "print(f\"  Prefetch: 2\")\n",
    "print(f\"  Persistent workers: True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a04382",
   "metadata": {},
   "source": [
    "## 7. Create Model (EfficientNet-B4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98ae8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create EfficientNet-B4 model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = timm.create_model('efficientnet_b4', pretrained=True, num_classes=num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"\\nğŸ¤– Model: EfficientNet-B4\")\n",
    "print(f\"ğŸ“Š Classes: {num_classes}\")\n",
    "print(f\"ğŸ–¥ï¸  Device: {device}\")\n",
    "print(f\"\\nâœ… Model ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68359c77",
   "metadata": {},
   "source": [
    "## 8. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f23cfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n",
    "\n",
    "epochs = 50\n",
    "best_val_acc = 0.0\n",
    "\n",
    "print(\"âš™ï¸  Training Configuration:\")\n",
    "print(f\"  Optimizer: AdamW (lr=0.0001, weight_decay=0.01)\")\n",
    "print(f\"  Scheduler: CosineAnnealingLR\")\n",
    "print(f\"  Loss: CrossEntropyLoss\")\n",
    "print(f\"  Epochs: {epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1bc147",
   "metadata": {},
   "source": [
    "## 9. Train Model (âš¡ Mixed Precision AMP)\n",
    "\n",
    "**Training optimizations:**\n",
    "- `torch.cuda.amp.autocast()` - FP16 mixed precision\n",
    "- `GradScaler` - prevents underflow\n",
    "- `zero_grad(set_to_none=True)` - faster gradient reset\n",
    "- `non_blocking=True` - async data transfer\n",
    "- Real-time ETA tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e3b7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš¡ OPTIMIZED Training loop with Mixed Precision (AMP)\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸš€ STARTING OPTIMIZED TRAINING (Mixed Precision + cuDNN)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import time\n",
    "epoch_times = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)  # âš¡ Faster than zero_grad()\n",
    "        \n",
    "        # âš¡ Mixed Precision Forward Pass\n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # âš¡ Scaled Backward Pass\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{train_loss/len(pbar):.4f}', \n",
    "                         'acc': f'{100.*train_correct/train_total:.2f}%'})\n",
    "    \n",
    "    train_acc = 100. * train_correct / train_total\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]  \"):\n",
    "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            \n",
    "            # âš¡ Mixed Precision for validation too\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    val_acc = 100. * val_correct / val_total\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'class_names': class_names,\n",
    "            'num_classes': num_classes\n",
    "        }, 'pakistan_model_best.pth')\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    epoch_times.append(epoch_time)\n",
    "    avg_time = sum(epoch_times) / len(epoch_times)\n",
    "    remaining = (epochs - epoch - 1) * avg_time / 60\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Epoch {epoch+1} Summary:\")\n",
    "    print(f\"   Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"   Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "    print(f\"   Best Val Acc: {best_val_acc:.2f}%\")\n",
    "    print(f\"   â±ï¸  Epoch time: {epoch_time:.1f}s | ETA: {remaining:.1f} min\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "total_time = sum(epoch_times) / 60\n",
    "print(f\"\\nâœ… Training complete in {total_time:.1f} minutes!\")\n",
    "print(f\"ğŸ† Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"âš¡ Average epoch time: {sum(epoch_times)/len(epoch_times):.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b0d57f",
   "metadata": {},
   "source": [
    "## 10. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e821f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load('pakistan_model_best.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Test evaluation\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "print(\"\\nğŸ§ª Evaluating on test set...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        test_total += labels.size(0)\n",
    "        test_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = 100. * test_correct / test_total\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š FINAL TEST RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nğŸ¯ Test Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"âœ… Correct: {test_correct:,} / {test_total:,}\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88abd700",
   "metadata": {},
   "source": [
    "## 11. Save Model and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d61f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete model info\n",
    "model_info = {\n",
    "    'class_names': class_names,\n",
    "    'num_classes': num_classes,\n",
    "    'test_accuracy': test_acc,\n",
    "    'best_val_accuracy': best_val_acc,\n",
    "    'crops': list(all_classes.keys()),\n",
    "    'total_images': len(all_image_paths),\n",
    "    'urdu_translations': {\n",
    "        'Rice': 'Ú†Ø§ÙˆÙ„',\n",
    "        'Wheat': 'Ú¯Ù†Ø¯Ù…',\n",
    "        'Cotton': 'Ú©Ù¾Ø§Ø³',\n",
    "        'Mango': 'Ø¢Ù…',\n",
    "        'Tomato': 'Ù¹Ù…Ø§Ù¹Ø±',\n",
    "        'Potato': 'Ø¢Ù„Ùˆ',\n",
    "        'Corn': 'Ù…Ú©Ø¦ÛŒ'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "with open('pakistan_model_info.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(model_info, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\nğŸ’¾ Files saved:\")\n",
    "print(\"  âœ“ pakistan_model_best.pth (model weights)\")\n",
    "print(\"  âœ“ pakistan_model_info.json (metadata)\")\n",
    "print(\"\\nğŸ“¥ Download these files from Kaggle output to use locally!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e282eff2",
   "metadata": {},
   "source": [
    "## 12. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e8387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(history['train_loss'], label='Train Loss')\n",
    "ax1.plot(history['val_loss'], label='Val Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(history['train_acc'], label='Train Accuracy')\n",
    "ax2.plot(history['val_acc'], label='Val Accuracy')\n",
    "ax2.axhline(y=test_acc, color='r', linestyle='--', label=f'Test Acc: {test_acc:.2f}%')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š Training visualization saved: training_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704082af",
   "metadata": {},
   "source": [
    "## âœ… Complete!\n",
    "\n",
    "### ğŸ“¥ Download from Kaggle Output:\n",
    "1. `pakistan_model_best.pth` (~200 MB)\n",
    "2. `pakistan_model_info.json` (~10 KB)\n",
    "3. `training_history.png`\n",
    "\n",
    "### ğŸš€ Use Locally:\n",
    "```python\n",
    "# Load model\n",
    "import torch\n",
    "import timm\n",
    "\n",
    "checkpoint = torch.load('pakistan_model_best.pth')\n",
    "model = timm.create_model('efficientnet_b4', num_classes=checkpoint['num_classes'])\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Use for predictions!\n",
    "```\n",
    "\n",
    "### Ú©Ø³Ø§Ù† Ù…Ø¯Ø¯Ú¯Ø§Ø± - Helping Pakistani Farmers with AI ğŸ‡µğŸ‡°"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
