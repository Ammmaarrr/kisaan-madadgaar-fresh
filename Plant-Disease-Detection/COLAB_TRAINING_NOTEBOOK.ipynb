{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ff6c00f",
   "metadata": {},
   "source": [
    "# üöÄ ⁄©ÿ≥ÿßŸÜ ŸÖÿØÿØ⁄Øÿßÿ± - Plant Disease Detection (Google Colab)\n",
    "\n",
    "## üìã Instructions:\n",
    "1. **Connect to Colab:** Select Kernel ‚Üí Google Colab ‚Üí T4 GPU\n",
    "2. **Run cells one by one** (Cell 1 ‚Üí Cell 10)\n",
    "3. **Download model** when training completes\n",
    "\n",
    "## üéØ This Notebook:\n",
    "- **Dataset:** PlantVillage COLOR folder only (38 classes)\n",
    "- **Model:** EfficientNet-B4 \n",
    "- **Target Accuracy:** 85-95%\n",
    "- **Auto-save:** Google Drive (prevents loss on disconnect)\n",
    "\n",
    "## ‚ö†Ô∏è Previous Issue Fixed:\n",
    "- OLD: 152 classes (color + grayscale + segmented = duplicates)\n",
    "- NEW: 38 unique classes (COLOR only) ‚Üí No overfitting!\n",
    "\n",
    "Let's begin! üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "539b59d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'Python 3 (ipykernel)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Unable to get resolved server information for google.colab:colab:5a8e3dd0-2384-4f63-b6e8-5c6b21606276"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 1: Setup Kaggle API (VS Code + Colab)\n",
    "# ============================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîß SETTING UP KAGGLE API (VS Code + Colab)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Kaggle credentials (from kaggle.json in repo)\n",
    "kaggle_credentials = {\n",
    "    \"username\": \"Ammmaaarrrr\",\n",
    "    \"key\": \"KGAT_448e734a3925f9b1cfc82a38e22445a0\"\n",
    "}\n",
    "\n",
    "# Setup kaggle directory\n",
    "kaggle_dir = Path.home() / \".kaggle\"\n",
    "kaggle_dir.mkdir(exist_ok=True)\n",
    "kaggle_json = kaggle_dir / \"kaggle.json\"\n",
    "\n",
    "# Write credentials\n",
    "with open(kaggle_json, 'w') as f:\n",
    "    json.dump(kaggle_credentials, f)\n",
    "\n",
    "try:\n",
    "    os.chmod(kaggle_json, 0o600)\n",
    "except:\n",
    "    pass  # Windows may not support chmod\n",
    "\n",
    "print(f\"‚úÖ Kaggle API configured at: {kaggle_json}\")\n",
    "\n",
    "# Set environment variables (alternative method)\n",
    "os.environ['KAGGLE_USERNAME'] = kaggle_credentials['username']\n",
    "os.environ['KAGGLE_KEY'] = kaggle_credentials['key']\n",
    "\n",
    "# Install kaggle CLI\n",
    "print(\"\\nüì¶ Installing kaggle CLI...\")\n",
    "import subprocess\n",
    "result = subprocess.run([\"pip\", \"install\", \"-q\", \"kaggle\"], capture_output=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Kaggle API setup complete!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fc8de2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'Python 3 (ipykernel)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Unable to get resolved server information for google.colab:colab:5a8e3dd0-2384-4f63-b6e8-5c6b21606276"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 2: Mount Drive & Download Dataset (COLOR ONLY)\n",
    "# ============================================\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìÇ DOWNLOADING PLANTVILLAGE DATASET (COLOR ONLY)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Mount Google Drive for auto-save\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    print(\"‚úÖ Google Drive mounted\")\n",
    "    DRIVE_SAVE = Path(\"/content/drive/MyDrive/trained_models\")\n",
    "    DRIVE_SAVE.mkdir(parents=True, exist_ok=True)\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Drive mount skipped\")\n",
    "    DRIVE_SAVE = Path(\"/content/saved_models\")\n",
    "    DRIVE_SAVE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download PlantVillage dataset\n",
    "print(\"\\nüì¶ Downloading PlantVillage dataset...\")\n",
    "os.system(\"kaggle datasets download -d abdallahalidev/plantvillage-dataset -p /content/data/ --unzip\")\n",
    "\n",
    "# ‚ö†Ô∏è USE ONLY COLOR FOLDER (38 unique classes)\n",
    "COLOR_DIR = Path(\"/content/data/plantvillage dataset/color\")\n",
    "DATA_DIR = Path(\"/content/data/plantvillage_color\")\n",
    "\n",
    "if COLOR_DIR.exists():\n",
    "    # Copy only color folder\n",
    "    if DATA_DIR.exists():\n",
    "        shutil.rmtree(DATA_DIR)\n",
    "    shutil.copytree(COLOR_DIR, DATA_DIR)\n",
    "    print(f\"‚úÖ Using COLOR folder only: {DATA_DIR}\")\n",
    "else:\n",
    "    # Try alternate path\n",
    "    alt_paths = [\n",
    "        Path(\"/content/data/PlantVillage/color\"),\n",
    "        Path(\"/content/data/plantvillage/color\"),\n",
    "    ]\n",
    "    for alt in alt_paths:\n",
    "        if alt.exists():\n",
    "            if DATA_DIR.exists():\n",
    "                shutil.rmtree(DATA_DIR)\n",
    "            shutil.copytree(alt, DATA_DIR)\n",
    "            print(f\"‚úÖ Using COLOR folder: {alt}\")\n",
    "            break\n",
    "    else:\n",
    "        DATA_DIR = Path(\"/content/data\")\n",
    "        print(\"‚ö†Ô∏è Color folder not found, using all data\")\n",
    "\n",
    "# Count classes\n",
    "class_folders = [f for f in DATA_DIR.iterdir() if f.is_dir()]\n",
    "total_images = 0\n",
    "\n",
    "print(f\"\\nüìä Found {len(class_folders)} classes:\")\n",
    "for folder in sorted(class_folders)[:10]:\n",
    "    count = len(list(folder.glob('*')))\n",
    "    total_images += count\n",
    "    print(f\"   üìÇ {folder.name}: {count:,} images\")\n",
    "\n",
    "if len(class_folders) > 10:\n",
    "    for folder in sorted(class_folders)[10:]:\n",
    "        total_images += len(list(folder.glob('*')))\n",
    "    print(f\"   ... and {len(class_folders) - 10} more classes\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total: {len(class_folders)} classes, {total_images:,} images\")\n",
    "print(f\"üíæ Auto-save path: {DRIVE_SAVE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b2015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîß SETTING UP KAGGLE API (VS Code + Colab)\n",
      "======================================================================\n",
      "‚úÖ Kaggle API configured at: /root/.kaggle/kaggle.json\n",
      "\n",
      "üì¶ Installing kaggle CLI...\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Kaggle API setup complete!\n",
      "======================================================================\n",
      "\n",
      "üì• Downloading 'rice' dataset...\n",
      "‚ùå Failed to download 'rice': \n",
      "‚ö†Ô∏è Check if Kaggle API is configured and dataset exists (404 if wrong).\n",
      "\n",
      "üì• Downloading 'cotton' dataset...\n",
      "‚ùå Failed to download 'cotton': \n",
      "‚ö†Ô∏è Check if Kaggle API is configured and dataset exists (404 if wrong).\n",
      "\n",
      "üì• Downloading 'mango' dataset...\n",
      "‚ùå Failed to download 'mango': \n",
      "‚ö†Ô∏è Check if Kaggle API is configured and dataset exists (404 if wrong).\n",
      "\n",
      "üì• Downloading 'wheat' dataset...\n",
      "‚ùå Failed to download 'wheat': \n",
      "‚ö†Ô∏è Check if Kaggle API is configured and dataset exists (404 if wrong).\n",
      "\n",
      "üìÇ Data folders in /content/data:\n",
      "   - plantvillage dataset\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 3: Install & Import Libraries\n",
    "# ============================================\n",
    "!pip install -q timm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "\n",
    "# Speed optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler = GradScaler()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Libraries imported!\")\n",
    "print(f\"üñ•Ô∏è Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a49e1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete!\n",
      "üñ•Ô∏è  Device: cuda\n",
      "üéÆ GPU: Tesla T4\n",
      "üíæ GPU Memory: 15.8 GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 4: Load Dataset (38 Classes)\n",
    "# ============================================\n",
    "print(\"=\"*70)\n",
    "print(\"üìä LOADING DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find all classes\n",
    "all_classes = {}\n",
    "image_extensions = {'.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG'}\n",
    "\n",
    "for class_folder in DATA_DIR.iterdir():\n",
    "    if class_folder.is_dir():\n",
    "        images = [str(f) for f in class_folder.iterdir() if f.suffix in image_extensions]\n",
    "        if len(images) > 0:\n",
    "            all_classes[class_folder.name] = images\n",
    "\n",
    "print(f\"‚úÖ Found {len(all_classes)} classes\")\n",
    "\n",
    "# Build dataset\n",
    "all_image_paths = []\n",
    "all_labels = []\n",
    "class_names = []\n",
    "\n",
    "for idx, (class_name, paths) in enumerate(sorted(all_classes.items())):\n",
    "    class_names.append(class_name)\n",
    "    all_image_paths.extend(paths)\n",
    "    all_labels.extend([idx] * len(paths))\n",
    "\n",
    "num_classes = len(class_names)\n",
    "print(f\"‚úÖ Total: {len(all_image_paths):,} images, {num_classes} classes\")\n",
    "\n",
    "# Show sample classes\n",
    "print(f\"\\nüìã Classes:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    count = sum(1 for l in all_labels if l == i)\n",
    "    print(f\"   {i+1:2d}. {name}: {count:,} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee2c220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä ANALYZING DATASETS\n",
      "======================================================================\n",
      "\n",
      "üîç Analyzing color...\n",
      "   Found: 38 classes, 54,305 images\n",
      "\n",
      "üîç Analyzing grayscale...\n",
      "   Found: 38 classes, 54,305 images\n",
      "\n",
      "üîç Analyzing segmented...\n",
      "   Found: 38 classes, 54,306 images\n",
      "\n",
      "======================================================================\n",
      "üìà TOTAL: 114 classes, 162,916 images\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 5: Create Dataset Class & Transforms\n",
    "# ============================================\n",
    "print(\"=\"*70)\n",
    "print(\"üîß CREATING DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class PlantDiseaseDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, self.labels[idx]\n",
    "        except:\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, self.labels[idx]\n",
    "\n",
    "# Strong augmentation to prevent overfitting\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.2)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Dataset class and transforms created\")\n",
    "print(\"üîÑ Using strong augmentation to prevent overfitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5275f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created dataset: 114 classes, 162,916 images\n",
      "\n",
      "üìã Sample classes:\n",
      "   1. color___Corn_(maize)___Common_rust_: 1192 images\n",
      "   2. color___Orange___Haunglongbing_(Citrus_greening): 5507 images\n",
      "   3. color___Pepper,_bell___Bacterial_spot: 997 images\n",
      "   4. color___Grape___Leaf_blight_(Isariopsis_Leaf_Spot): 1076 images\n",
      "   5. color___Corn_(maize)___healthy: 1162 images\n",
      "   ... and 109 more classes\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 6: Create Data Loaders\n",
    "# ============================================\n",
    "print(\"=\"*70)\n",
    "print(\"üìä CREATING DATA LOADERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create dataset\n",
    "full_dataset = PlantDiseaseDataset(all_image_paths, all_labels, train_transform)\n",
    "\n",
    "# Split: 70% train, 15% val, 15% test\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = int(0.15 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "train_ds, val_ds, test_ds = random_split(\n",
    "    full_dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Update val/test transforms (no augmentation)\n",
    "val_ds.dataset = PlantDiseaseDataset(all_image_paths, all_labels, test_transform)\n",
    "test_ds.dataset = PlantDiseaseDataset(all_image_paths, all_labels, test_transform)\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, \n",
    "                          num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, \n",
    "                        num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, \n",
    "                         num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"\\nüìä Dataset Split:\")\n",
    "print(f\"   Training:   {len(train_ds):,} images ({len(train_loader)} batches)\")\n",
    "print(f\"   Validation: {len(val_ds):,} images ({len(val_loader)} batches)\")\n",
    "print(f\"   Test:       {len(test_ds):,} images ({len(test_loader)} batches)\")\n",
    "print(f\"\\n‚ö° Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4da7af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Dataset Split:\n",
      "  Training:   114,041 images\n",
      "  Validation: 24,437 images\n",
      "  Test:       24,438 images\n",
      "\n",
      "‚ö° Optimizations: batch=64, workers=4, pin_memory=True\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 7: Create Model (EfficientNet-B4 with Dropout)\n",
    "# ============================================\n",
    "print(\"=\"*70)\n",
    "print(\"ü§ñ CREATING MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# EfficientNet-B4 with dropout for regularization\n",
    "model = timm.create_model('efficientnet_b4', pretrained=True, num_classes=num_classes, drop_rate=0.3)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"ü§ñ Model: EfficientNet-B4\")\n",
    "print(f\"üìä Classes: {num_classes}\")\n",
    "print(f\"‚öôÔ∏è Parameters: {total_params:,}\")\n",
    "print(f\"üéØ Dropout: 0.3 (prevents overfitting)\")\n",
    "\n",
    "# Loss, optimizer, scheduler\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Label smoothing helps!\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0003, weight_decay=0.05)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Optimizer: AdamW (lr=0.0003, weight_decay=0.05)\")\n",
    "print(f\"üìÖ Scheduler: CosineAnnealingWarmRestarts\")\n",
    "print(f\"üè∑Ô∏è Label Smoothing: 0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d051cd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ STARTING TRAINING (Mixed Precision + cuDNN)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1782/1782 [13:49<00:00,  2.15it/s, loss=0.1616, acc=84.4%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 1: Train Loss=0.5969, Train Acc=84.43%\n",
      "   Val Loss=0.1155, Val Acc=95.94%, Best=95.94%\n",
      "   ‚è±Ô∏è Time: 931s | ETA: 372.6 min\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1782/1782 [11:59<00:00,  2.48it/s, loss=0.1702, acc=96.6%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 2: Train Loss=0.0930, Train Acc=96.59%\n",
      "   Val Loss=0.0804, Val Acc=97.12%, Best=97.12%\n",
      "   ‚è±Ô∏è Time: 807s | ETA: 309.4 min\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1782/1782 [11:59<00:00,  2.48it/s, loss=0.0418, acc=97.9%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 3: Train Loss=0.0536, Train Acc=97.95%\n",
      "   Val Loss=0.0602, Val Acc=97.75%, Best=97.75%\n",
      "   ‚è±Ô∏è Time: 805s | ETA: 295.2 min\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1782/1782 [11:59<00:00,  2.48it/s, loss=0.0455, acc=98.6%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 4: Train Loss=0.0380, Train Acc=98.56%\n",
      "   Val Loss=0.0560, Val Acc=98.02%, Best=98.02%\n",
      "   ‚è±Ô∏è Time: 805s | ETA: 281.7 min\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1782/1782 [11:59<00:00,  2.48it/s, loss=0.0487, acc=98.9%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 5: Train Loss=0.0280, Train Acc=98.90%\n",
      "   Val Loss=0.0519, Val Acc=98.13%, Best=98.13%\n",
      "   ‚è±Ô∏è Time: 800s | ETA: 266.8 min\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1782/1782 [11:58<00:00,  2.48it/s, loss=0.0255, acc=99.1%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 6: Train Loss=0.0230, Train Acc=99.10%\n",
      "   Val Loss=0.0449, Val Acc=98.41%, Best=98.41%\n",
      "   ‚è±Ô∏è Time: 804s | ETA: 254.6 min\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1782/1782 [11:57<00:00,  2.48it/s, loss=0.0213, acc=99.2%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 7: Train Loss=0.0192, Train Acc=99.24%\n",
      "   Val Loss=0.0466, Val Acc=98.33%, Best=98.41%\n",
      "   ‚è±Ô∏è Time: 797s | ETA: 239.0 min\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1782/1782 [11:58<00:00,  2.48it/s, loss=0.0325, acc=99.3%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 8: Train Loss=0.0175, Train Acc=99.32%\n",
      "   Val Loss=0.0456, Val Acc=98.50%, Best=98.50%\n",
      "   ‚è±Ô∏è Time: 800s | ETA: 226.7 min\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1782/1782 [11:57<00:00,  2.48it/s, loss=0.0091, acc=99.4%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 9: Train Loss=0.0148, Train Acc=99.40%\n",
      "   Val Loss=0.0434, Val Acc=98.49%, Best=98.50%\n",
      "   ‚è±Ô∏è Time: 796s | ETA: 212.4 min\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1782/1782 [11:57<00:00,  2.48it/s, loss=0.0060, acc=99.5%]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://8080-gpu-t4-s-2yw3llrarefdf-a.asia-southeast1-0.prod.colab.dev/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 8: Training Loop (15 Epochs + Early Stopping)\n",
    "# ============================================\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ STARTING TRAINING (15 Epochs)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "epochs = 15\n",
    "best_val_acc = 0.0\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "print(f\"üíæ Auto-save to: {DRIVE_SAVE}\")\n",
    "print(f\"‚è∞ Early stopping patience: {patience}\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss, train_correct, train_total = 0, 0, 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.3f}', 'acc': f'{100.*train_correct/train_total:.1f}%'})\n",
    "    \n",
    "    train_acc = 100. * train_correct / train_total\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    val_acc = 100. * val_correct / val_total\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'class_names': class_names,\n",
    "            'num_classes': num_classes,\n",
    "            'history': history\n",
    "        }\n",
    "        torch.save(checkpoint, 'pakistan_model_best.pth')\n",
    "        # Auto-save to Drive\n",
    "        try:\n",
    "            shutil.copy('pakistan_model_best.pth', DRIVE_SAVE / 'pakistan_model_best.pth')\n",
    "            print(f\"   üíæ Saved to Drive! (Val: {val_acc:.2f}%)\")\n",
    "        except:\n",
    "            print(f\"   üíæ Saved locally! (Val: {val_acc:.2f}%)\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Save class_names and history every epoch\n",
    "    with open('class_names.json', 'w') as f:\n",
    "        json.dump(class_names, f, indent=2)\n",
    "    with open('training_history.json', 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    try:\n",
    "        shutil.copy('class_names.json', DRIVE_SAVE / 'class_names.json')\n",
    "        shutil.copy('training_history.json', DRIVE_SAVE / 'training_history.json')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    scheduler.step()\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"\\nüìä Epoch {epoch+1}: Train={train_acc:.1f}%, Val={val_acc:.1f}%, Best={best_val_acc:.1f}%\")\n",
    "    print(f\"   ‚è±Ô∏è {elapsed:.0f}s | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\n‚èπÔ∏è Early stopping! No improvement for {patience} epochs\")\n",
    "        break\n",
    "    print(\"-\"*70)\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete! Best: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2c12e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üö® RECOVERY: Saving partial training progress\n",
      "======================================================================\n",
      "\n",
      "üìä Epochs completed: 0\n",
      "‚ùå No training history found. Training may not have started.\n",
      "   Run Cell 8 to start training first.\n",
      "\n",
      "======================================================================\n",
      "üîÑ To RESUME training from checkpoint, run Cell 8.6 below\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 9: Test & Plot Results\n",
    "# ============================================\n",
    "print(\"=\"*70)\n",
    "print(\"üß™ TESTING & PLOTTING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load('pakistan_model_best.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Test\n",
    "test_correct, test_total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "test_acc = 100. * test_correct / test_total\n",
    "print(f\"\\nüéØ Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "axes[0].plot(epochs_range, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "axes[0].plot(epochs_range, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(epochs_range, history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
    "axes[1].plot(epochs_range, history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
    "axes[1].axhline(y=test_acc, color='g', linestyle='--', label=f'Test: {test_acc:.1f}%')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Results saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d5c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîÑ RESUMING TRAINING FROM CHECKPOINT\n",
      "======================================================================\n",
      "‚ùå No checkpoint found. Starting from scratch.\n",
      "\n",
      "üöÄ Continuing training for 25 more epochs...\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1240136301.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Training phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CELL 10: Download Model Files\n",
    "# ============================================\n",
    "print(\"=\"*70)\n",
    "print(\"üì• DOWNLOADING MODEL FILES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save final info\n",
    "model_info = {\n",
    "    'class_names': class_names,\n",
    "    'num_classes': num_classes,\n",
    "    'test_accuracy': test_acc,\n",
    "    'best_val_accuracy': best_val_acc,\n",
    "    'model_architecture': 'efficientnet_b4',\n",
    "    'total_images': len(all_image_paths),\n",
    "    'epochs_trained': len(history['train_loss'])\n",
    "}\n",
    "\n",
    "with open('model_info.json', 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "# Copy to Drive\n",
    "try:\n",
    "    shutil.copy('pakistan_model_best.pth', DRIVE_SAVE / 'pakistan_model_best.pth')\n",
    "    shutil.copy('class_names.json', DRIVE_SAVE / 'class_names.json')\n",
    "    shutil.copy('model_info.json', DRIVE_SAVE / 'model_info.json')\n",
    "    shutil.copy('training_history.json', DRIVE_SAVE / 'training_history.json')\n",
    "    shutil.copy('training_history.png', DRIVE_SAVE / 'training_history.png')\n",
    "    print(f\"‚úÖ All files saved to Google Drive: {DRIVE_SAVE}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Drive save failed: {e}\")\n",
    "\n",
    "# Download files\n",
    "print(\"\\nüì• Downloading files...\")\n",
    "from google.colab import files\n",
    "\n",
    "files.download('pakistan_model_best.pth')\n",
    "files.download('class_names.json')\n",
    "files.download('model_info.json')\n",
    "files.download('training_history.json')\n",
    "files.download('training_history.png')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ DONE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìä Final Results:\")\n",
    "print(f\"   üèÜ Test Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"   üèÜ Best Val Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"   üìÅ Classes: {num_classes}\")\n",
    "print(f\"\\nüìÅ Files downloaded - copy to 'Flask Deployed App/' folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61bb90f",
   "metadata": {},
   "source": [
    "## ‚úÖ Training Complete!\n",
    "\n",
    "### üìÅ Downloaded Files:\n",
    "- `pakistan_model_best.pth` - Model weights\n",
    "- `class_names.json` - 38 class labels\n",
    "- `model_info.json` - Metadata\n",
    "- `training_history.json` - Training log\n",
    "- `training_history.png` - Training plot\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. Copy files to `Flask Deployed App/` folder\n",
    "2. Run Flask app: `python app.py`\n",
    "3. Test with plant images!\n",
    "\n",
    "### üáµüá∞ ⁄©ÿ≥ÿßŸÜ ŸÖÿØÿØ⁄Øÿßÿ± - Helping Pakistani Farmers!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
